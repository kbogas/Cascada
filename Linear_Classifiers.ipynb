{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classifiers\n",
    "Use some kind of random patches and train multiple classifiers on very small parts of the dataset. Specifically, for each data sample, sample from it's nearst sameclass and diff/class samples and create a small neighborhood. THen create a linear classifier based on the normal equation for fast inference.\n",
    "Finally, find a smart way to combine these results (each model will occupy a specific point that would be the centroid of it's neighborhood. then we can use the distance as weights when predicting for a new sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_selected_points(clf, X, y, center_id, indices, s=100):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = plt.gca()\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1)\n",
    "    plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=s, edgecolors='grey')\n",
    "    ax.scatter(X[center_id,0], X[center_id,1], s=s, edgecolors='green', facecolors='none')\n",
    "    ax.scatter(X[indices,0], X[indices, 1], s=s, edgecolors='black', facecolors='none')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    #print(clf)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "C = 1.0  # SVM regularization parameter\n",
    "base =  AdaBoostClassifier()\n",
    "        #KNeighborsClassifier()        \n",
    "        #ExtraTreeClassifier()\n",
    "        #DecisionTreeClassifier()\n",
    "        #SGDClassifier() \n",
    "        #svm.SVC(kernel='linear', C=C, probability=True)\n",
    "bagging = BaggingClassifier(base, max_samples=0.5, max_features=0.5, random_state=42)\n",
    "rp = Vanilla_RP(base,max_samples=0.5, max_features=0.5, random_state=42)\n",
    "rp_w = Vanilla_RP(base, max_samples=0.5, max_features=0.5, patcher='weighted', random_state=42)\n",
    "ac = Adversarial_Cascade(base_estimator=base, num_adversaries_per_instance=10,\n",
    "                         optim=False, \n",
    "                         parameters=None, oob=True, way='furthest')\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C), \n",
    "          KNeighborsClassifier(),\n",
    "         pipeline,\n",
    "         bagging,\n",
    "         rp,\n",
    "         rp_w,\n",
    "         ac)\n",
    "\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'KNN',\n",
    "         'SGD', \n",
    "         \"Bagging\",\n",
    "         \"RP\",\n",
    "         \"RP-W\",\n",
    "         'AC')\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 4)\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    \n",
    "    print(\"%s : %0.3f\"% (title, 100*accuracy_score(y, clf.predict(X))))\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Classifiers(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, base_estimator=KNeighborsClassifier(), n_estimators=10, acc_target=0.99,\n",
    "                 num_adversaries_per_instance=4, way = 'prob', \n",
    "                 random_state=42, optim=False, parameters=None, metric='accuracy', oob=False, oob_size=0.1):\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.acc_target = acc_target\n",
    "        self.num_adversaries_per_instance = num_adversaries_per_instance\n",
    "        self.way = way\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        self.optim = optim\n",
    "        self.oob = oob\n",
    "        self.oob_size = 0.1\n",
    "        self.X_oob = None\n",
    "        self.y_oob = None\n",
    "        if self.optim:\n",
    "            self.parameters = parameters\n",
    "        else:\n",
    "            self.parameters = None\n",
    "        self.scoring = get_scorer(metric)\n",
    "        self.acc = 0\n",
    "        self.ensemble = []\n",
    "        self.selected_indices = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        return self._fit(X, y)\n",
    "    \n",
    "    def _fit(self,X,y):\n",
    "        X, y = check_X_y(\n",
    "            X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,\n",
    "            multi_output=True)\n",
    "        y = self._validate_y(y)\n",
    "        if self.oob:\n",
    "            X, self.X_oob, y, self.y_oob = train_test_split(X,y,test_size=0.1,stratify=y)\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        cur_X, cur_y = X, y\n",
    "        self.selected_indices.append([i for i in xrange(X.shape[0])])\n",
    "        flag_target = False\n",
    "        for i_est in xrange(self.n_estimators):\n",
    "            cur_mod = clone(self.base_estimator)\n",
    "            if self.optim:\n",
    "                grid_search = GridSearchCV(cur_mod, self.parameters, n_jobs=-1, verbose=1, refit=True)\n",
    "                grid_search.fit(cur_X, cur_y)\n",
    "                cur_mod = grid_search.best_estimator_\n",
    "            else:\n",
    "                cur_mod.fit(cur_X, cur_y)\n",
    "            self.ensemble.append(cur_mod)\n",
    "            cur_X, cur_y, flag_target = self._create_next_batch(X, y)\n",
    "            if flag_target:\n",
    "                break\n",
    "            #print(cur_X.shape, cur_y.shape)\n",
    "        print(\"%d ESTIMATORS -- %0.3f\" % (len(self.ensemble), 100*accuracy_score(y, self.predict(X), normalize=True)))\n",
    "        return self\n",
    "    \n",
    "    def _create_next_batch(self, X, y):\n",
    "        if self.oob:\n",
    "            preds = self.predict(self.X_oob)\n",
    "            centroids = self.X_oob[preds != self.y_oob]\n",
    "            centroids_ind = np.argwhere(preds != self.y_oob).reshape(-1,)\n",
    "            cur_X = copy.deepcopy(self.X_oob[centroids_ind,:])\n",
    "            cur_y = copy.deepcopy(self.y_oob[centroids_ind])\n",
    "            str_target = \"OOB SAMPLE\"\n",
    "            self.acc = accuracy_score(self.y_oob, preds, normalize=True)\n",
    "            #acc = (1-(centroids.shape[0])/float(self.X_oob.shape[0]))\n",
    "        else:\n",
    "            preds = self.predict(X)\n",
    "            centroids = X[preds != y]\n",
    "            centroids_ind = np.argwhere(preds!=y).reshape(-1,)\n",
    "            cur_X = copy.deepcopy(X[centroids_ind,:])\n",
    "            cur_y = copy.deepcopy(y[centroids_ind])\n",
    "            str_target = \"TRAIN SAMPLE\"\n",
    "            self.acc = accuracy_score(y, preds, normalize=True)\n",
    "            #acc = (1-(centroids.shape[0])/float(X.shape[0]))\n",
    "        if  self.acc > self.acc_target:\n",
    "            #return X, y, False\n",
    "            #print(\"ACCURACY ON THE %s IS %0.3f\" % (str_target, 100*(1-(centroids.shape[0])/float(X.shape[0]))))\n",
    "            #print(\"STOPPING WITH %d BASE MODELS\" % len(self.ensemble))\n",
    "            return _, _,True\n",
    "        probas = pairwise_distances(centroids, X)\n",
    "        probas /= np.sum(probas, axis=1).reshape(-1,1)\n",
    "        for i_centr in xrange(probas.shape[0]):\n",
    "            # Make zero the probability that a same-class sample is picked\n",
    "            cur_prob = copy.deepcopy(probas[i_centr,:])\n",
    "            cur_prob[y[centroids_ind[i_centr]]==y]=0\n",
    "            print(cur_prob.shape, np.sum(cur_prob))\n",
    "            cur_prob /= np.sum(cur_prob)\n",
    "            if self.way == 'prob':\n",
    "                indices = self.random_state.choice([i for i in xrange(0, probas.shape[1])],\n",
    "                                                   self.num_adversaries_per_instance, p=cur_prob)\n",
    "            if self.way == 'furthest':\n",
    "                indices = np.argsort(cur_prob)[::-1][:self.num_adversaries_per_instance]\n",
    "            if self.way == 'closest':\n",
    "                cur_prob[y[centroids_ind[i_centr]]==y]=1\n",
    "                indices = np.argsort(cur_prob)[:self.num_adversaries_per_instance]\n",
    "            indices = self._fix_class_indices(y, indices)\n",
    "            #print(cur_X.shape, X[indices,:].shape)\n",
    "            cur_X = np.vstack((cur_X, X[indices,:]))\n",
    "            cur_y = np.append(cur_y, y[indices])\n",
    "                #cur_y.extend(indices)\n",
    "\n",
    "            #cur_X = np.delete(cur_X, 0, axis=0)\n",
    "            #cur_y = y[cur_y]\n",
    "        return cur_X, cur_y, False\n",
    "        \n",
    "    def _fix_class_indices(self, y, samples_indices):\n",
    "        in_set = set(y[samples_indices])\n",
    "        a = set(y).difference(in_set)\n",
    "        for item in a:\n",
    "           samples_indices= np.append(samples_indices, [np.where(y==item)[0][0]])\n",
    "        return samples_indices    \n",
    "    \n",
    "    def _validate_y(self, y):\n",
    "        y = column_or_1d(y, warn=True)\n",
    "        check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class for X.\n",
    "        The predicted class of an input sample is computed as the class with\n",
    "        the highest mean predicted probability. If base estimators do not\n",
    "        implement a ``predict_proba`` method, then it resorts to voting.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        \n",
    "        if hasattr(self.base_estimator, \"predict_proba\"):\n",
    "            predicted_probability = self.predict_proba(X)\n",
    "            return self.classes_.take((np.argmax(predicted_probability, axis=1)),\n",
    "                                  axis=0)\n",
    "        else:\n",
    "            predicted_probability = np.zeros((X.shape[0],1), dtype=int)\n",
    "            for i, ens in enumerate(self.ensemble):\n",
    "                predicted_probability = np.hstack((predicted_probability, ens.predict(X).reshape(-1,1)))\n",
    "            predicted_probability = np.delete(predicted_probability,0,axis=1)\n",
    "            final_pred = []\n",
    "            for sample in xrange(X.shape[0]):\n",
    "                final_pred.append(most_common(predicted_probability[sample,:]))\n",
    "                #votes = []\n",
    "                #for i, mod_vote in predictions[sample,:]:\n",
    "                #    votes.extend([predictions[sample, i] for j in xrange(int(self.acc[i]))])\n",
    "                #final_pred = most_common(votes)\n",
    "            return np.array(final_pred)   \n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities for X.\n",
    "        The predicted class probabilities of an input sample is computed as\n",
    "        the mean predicted class probabilities of the base estimators in the\n",
    "        ensemble. If base estimators do not implement a ``predict_proba``\n",
    "        method, then it resorts to voting and the predicted class probabilities\n",
    "        of an input sample represents the proportion of estimators predicting\n",
    "        each class.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_classes]\n",
    "            The class probabilities of the input samples. The order of the\n",
    "            classes corresponds to that in the attribute `classes_`.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "        # Check data\n",
    "        X = check_array(\n",
    "            X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False\n",
    "        )\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1}.\"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "\n",
    "        all_proba = np.zeros((X.shape[0], self.n_classes_))\n",
    "        for i, ens in enumerate(self.ensemble):\n",
    "            all_proba += ens.predict_proba(X)\n",
    "        all_proba /= self.n_estimators\n",
    "        #print(all_proba.shape)\n",
    "        #print(all_proba)\n",
    "        #proba = np.sum(all_proba, axis=0) / self.n_estimators\n",
    "        #print(proba.shape)\n",
    "        #print(proba)\n",
    "        return all_proba\n",
    "\n",
    "\n",
    "    @if_delegate_has_method(delegate='base_estimator')\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"Average of the decision functions of the base classifiers.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n",
    "            The training input samples. Sparse matrices are accepted only if\n",
    "            they are supported by the base estimator.\n",
    "        Returns\n",
    "        -------\n",
    "        score : array, shape = [n_samples, k]\n",
    "            The decision function of the input samples. The columns correspond\n",
    "            to the classes in sorted order, as they appear in the attribute\n",
    "            ``classes_``. Regression and binary classification are special\n",
    "            cases with ``k == 1``, otherwise ``k==n_classes``.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"classes_\")\n",
    "\n",
    "        # Check data\n",
    "        X = check_array(\n",
    "            X, accept_sparse=['csr', 'csc'], dtype=None,\n",
    "            force_all_finite=False\n",
    "        )\n",
    "\n",
    "        if self.n_features_ != X.shape[1]:\n",
    "            raise ValueError(\"Number of features of the model must \"\n",
    "                             \"match the input. Model n_features is {0} and \"\n",
    "                             \"input n_features is {1} \"\n",
    "                             \"\".format(self.n_features_, X.shape[1]))\n",
    "        all_decisions = np.zeros((X.shape[0], self.n_classes_))\n",
    "        for i, ens in enumerate(self.ensemble):\n",
    "            all_decisions += ens.predict_proba(X) \n",
    "        decisions = sum(all_decisions) / self.n_estimators\n",
    "\n",
    "        return decisions\n",
    "    \n",
    "    def viz_fit(self, X, y):\n",
    "        X, y = check_X_y(\n",
    "            X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,\n",
    "            multi_output=True)\n",
    "        y = self._validate_y(y)\n",
    "        if self.oob:\n",
    "            X, self.X_oob, y, self.y_oob = train_test_split(X,y,test_size=0.1,stratify=y)\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        cur_X, cur_y = X, y\n",
    "        self.selected_indices.append([i for i in xrange(X.shape[0])])\n",
    "        flag_target = False\n",
    "        for i_est in xrange(self.n_estimators):\n",
    "            cur_mod = clone(self.base_estimator)\n",
    "            if self.optim:\n",
    "                grid_search = GridSearchCV(cur_mod, self.parameters, n_jobs=-1, verbose=1, refit=True)\n",
    "                grid_search.fit(cur_X, cur_y)\n",
    "                cur_mod = grid_search.best_estimator_\n",
    "            else:\n",
    "                cur_mod.fit(cur_X, cur_y)\n",
    "            self.ensemble.append(cur_mod)\n",
    "            cur_X, cur_y, flag_target = self.viz_create_next_batch(X, y)\n",
    "            if flag_target:\n",
    "                break\n",
    "            #print(cur_X.shape, cur_y.shape)\n",
    "        print(\"%d ESTIMATORS -- %0.3f\" % (len(self.ensemble), 100*accuracy_score(y, self.predict(X), normalize=True)))\n",
    "        return self\n",
    "    \n",
    "    def viz_create_next_batch(self, X, y):\n",
    "        if self.oob:\n",
    "            preds = self.predict(self.X_oob)\n",
    "            centroids = self.X_oob[preds != self.y_oob]\n",
    "            centroids_ind = np.argwhere(preds != self.y_oob).reshape(-1,)\n",
    "            cur_X = copy.deepcopy(self.X_oob[centroids_ind,:])\n",
    "            cur_y = copy.deepcopy(self.y_oob[centroids_ind])\n",
    "            str_target = \"OOB SAMPLE\"\n",
    "            self.acc = accuracy_score(self.y_oob, preds, normalize=True)\n",
    "            #acc = (1-(centroids.shape[0])/float(self.X_oob.shape[0]))\n",
    "        else:\n",
    "            preds = self.predict(X)\n",
    "            centroids = X[preds != y]\n",
    "            centroids_ind = np.argwhere(preds!=y).reshape(-1,)\n",
    "            cur_X = copy.deepcopy(X[centroids_ind,:])\n",
    "            cur_y = copy.deepcopy(y[centroids_ind])\n",
    "            str_target = \"TRAIN SAMPLE\"\n",
    "            self.acc = accuracy_score(y, preds, normalize=True)\n",
    "            #acc = (1-(centroids.shape[0])/float(X.shape[0]))\n",
    "        if  self.acc > self.acc_target:\n",
    "            #return X, y, False\n",
    "            #print(\"ACCURACY ON THE %s IS %0.3f\" % (str_target, 100*(1-(centroids.shape[0])/float(X.shape[0]))))\n",
    "            #print(\"STOPPING WITH %d BASE MODELS\" % len(self.ensemble))\n",
    "            return _, _,True\n",
    "        probas = pairwise_distances(centroids, X)\n",
    "        probas /= np.sum(probas, axis=1).reshape(-1,1)\n",
    "        for i_centr in xrange(probas.shape[0]):\n",
    "            # Make zero the probability that a same-class sample is picked\n",
    "            cur_prob = copy.deepcopy(probas[i_centr,:])\n",
    "            cur_prob[y[centroids_ind[i_centr]]==y]=0\n",
    "            print(cur_prob.shape, np.sum(cur_prob))\n",
    "            cur_prob /= np.sum(cur_prob)\n",
    "            if self.way == 'prob':\n",
    "                indices = self.random_state.choice([i for i in xrange(0, probas.shape[1])],\n",
    "                                                   self.num_adversaries_per_instance, p=cur_prob)\n",
    "            if self.way == 'furthest':\n",
    "                indices = np.argsort(cur_prob)[::-1][:self.num_adversaries_per_instance]\n",
    "            if self.way == 'closest':\n",
    "                cur_prob[y[centroids_ind[i_centr]]==y]=1\n",
    "                indices = np.argsort(cur_prob)[:self.num_adversaries_per_instance]\n",
    "            indices = self._fix_class_indices(y, indices)\n",
    "            \n",
    "            #print(cur_X.shape, X[indices,:].shape)\n",
    "            cur_X = np.vstack((cur_X, X[indices,:]))\n",
    "            cur_y = np.append(cur_y, y[indices])\n",
    "                #cur_y.extend(indices)\n",
    "\n",
    "            #cur_X = np.delete(cur_X, 0, axis=0)\n",
    "            #cur_y = y[cur_y]\n",
    "        plot_selected_points(self, X,y, centroids_ind[i_centr], indices)\n",
    "        cc = raw_input()\n",
    "        if cc == 'q':\n",
    "            exit\n",
    "        return cur_X, cur_y, False\n",
    "       \n",
    "\n",
    "def most_common(lst):\n",
    "    if isinstance(lst, np.ndarray):\n",
    "        lst = lst.tolist()\n",
    "    #print(lst, max(set(lst), key=lst.count)  )\n",
    "    return max(set(lst), key=lst.count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
